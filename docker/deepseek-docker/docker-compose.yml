version: '3.4'
 

services:
    openwebui:
        image: ghcr.io/open-webui/open-webui:main
        restart: unless-stopped
        depends_on:
            - ollamadeepseek
        ports:
           - '8333:8080'
        volumes:
          - './open-webui:/app/backend/data'
        environment:
            OLLAMA_BASE_URL: http://ollamadeepseek:11434 
    ollamadeepseek: 
        build:
            context: .
            dockerfile: Dockerfile
        restart: unless-stopped
        ports:
           - '11434:11434'
        volumes:
          - './ollama:/root/.ollama'
        environment:
            OLLAMA_HOST: 0.0.0.0
            OLLAMA_PORT: 11434
            OLLAMA_MODELS: 
        #  To use only CPU, Just  comment the following line or remove it
        deploy:
          resources:
            reservations:
              devices:
                - driver: nvidia
                  count: all
                  capabilities: [gpu]
